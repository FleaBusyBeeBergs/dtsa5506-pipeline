{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFYp2XYevgh7ulxtqjEHf9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FleaBusyBeeBergs/dtsa5506-pipeline/blob/main/statcan_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7RdJsRYMM3DG"
      },
      "outputs": [],
      "source": [
        "# file handling\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# http requests\n",
        "import requests\n",
        "\n",
        "# vis\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETL pipeline"
      ],
      "metadata": {
        "id": "XvVhozpExa_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stats can has tools to make http requests for date from their cube. [1]"
      ],
      "metadata": {
        "id": "7DkGmi9Lx17v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base URL for data extraction\n",
        "BASE_URL = 'https://www150.statcan.gc.ca/t1/wds/sdmx/statcan/rest/vector/'"
      ],
      "metadata": {
        "id": "J4Kn2gIPM-9R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I was able to make a request for a single vector, but needed help with transforming the XML output that was extracted. I spent a lot of time trying to write a loop to extract and parse multiple vectors, and ended up using a code generator [2] for the following StatCanETL class:"
      ],
      "metadata": {
        "id": "xsTyYzO1yjYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Namespace mappings for XML parsing\n",
        "NAMESPACES = {\n",
        "    \"message\": \"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/message\",\n",
        "    \"generic\": \"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/data/generic\",\n",
        "    \"common\": \"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/common\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "W-Udwi4MPqMW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StatCanETL:\n",
        "    def __init__(self, table_df):\n",
        "        # Initialize with the DataFrame of table metadata\n",
        "        self.table_df = table_df\n",
        "        self.data_objects = {}  # Dictionary to store DataFrames dynamically\n",
        "\n",
        "    def extract(self, vector, start_period, end_period):\n",
        "        \"\"\"Extract data from the StatCan API for a given vector.\"\"\"\n",
        "        url = f\"{BASE_URL}{vector}?startPeriod={start_period}&endPeriod={end_period}&detail=full\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.content\n",
        "        else:\n",
        "            raise ValueError(f\"Failed to fetch data for vector {vector}. HTTP Status: {response.status_code}\")\n",
        "\n",
        "    def transform(self, xml_content, frequency):\n",
        "        \"\"\"Transform the XML response into a structured DataFrame.\"\"\"\n",
        "        # Parse XML content\n",
        "        root = ET.fromstring(xml_content)\n",
        "\n",
        "        # Locate the Series element\n",
        "        series = root.find(\".//generic:Series\", NAMESPACES)\n",
        "        if series is None:\n",
        "            raise ValueError(\"No Series element found in the XML response.\")\n",
        "\n",
        "        # Extract observations\n",
        "        observations = series.findall(\".//generic:Obs\", NAMESPACES)\n",
        "        data = []\n",
        "        for obs in observations:\n",
        "            obs_dim = obs.find(\".//generic:ObsDimension\", NAMESPACES)\n",
        "            obs_val = obs.find(\".//generic:ObsValue\", NAMESPACES)\n",
        "            date = obs_dim.attrib[\"value\"]\n",
        "\n",
        "            # Convert date format if the frequency is quarterly\n",
        "            if frequency == \"quarterly\":\n",
        "                date = self.convert_quarter_to_date(date)\n",
        "\n",
        "            data.append({\n",
        "                \"Date\": date,\n",
        "                \"Value\": float(obs_val.attrib[\"value\"]),\n",
        "            })\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_quarter_to_date(quarter_str):\n",
        "        \"\"\"Convert a quarterly date string (YYYY-Qx) to a standard date format (YYYY-MM-DD).\"\"\"\n",
        "        try:\n",
        "            year, quarter = quarter_str.split(\"-Q\")\n",
        "            quarter_start_month = {\n",
        "                \"1\": \"01\",\n",
        "                \"2\": \"04\",\n",
        "                \"3\": \"07\",\n",
        "                \"4\": \"10\",\n",
        "            }.get(quarter)\n",
        "            if quarter_start_month:\n",
        "                return f\"{year}-{quarter_start_month}\"\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid quarter format: {quarter_str}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error converting quarter string {quarter_str}: {e}\")\n",
        "\n",
        "    def load(self, df, name):\n",
        "        \"\"\"Load the DataFrame into an object named name_df.\"\"\"\n",
        "        variable_name = f\"{name}_df\"\n",
        "        globals()[variable_name] = df  # Dynamically create a global variable\n",
        "        self.data_objects[variable_name] = df  # Store in a dictionary for easy access\n",
        "        print(f\"Data for {name} saved to object {variable_name}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the full ETL pipeline for all variables.\"\"\"\n",
        "        for _, row in self.table_df.iterrows():\n",
        "            print(f\"Processing {row['name']}...\")\n",
        "            try:\n",
        "                # Extract\n",
        "                xml_content = self.extract(row[\"vector\"], row[\"start\"], row[\"end\"])\n",
        "\n",
        "                # Transform\n",
        "                df = self.transform(xml_content, row[\"frequency\"])\n",
        "\n",
        "                # Load\n",
        "                self.load(df, row[\"name\"])\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {row['name']}: {e}\")"
      ],
      "metadata": {
        "id": "ictARtWmPwGh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the table metadata\n",
        "tables = {\n",
        "    'name': ['wage', 'raw', 'tax', 'productivity', 'cpi'],\n",
        "    'tableid': [14100223, 18100268, 11100058, 36100206, 18100004],\n",
        "    'vector': ['v79311153', 'v1230998135', 'v122807833', 'v1409153', 'v41690973'],\n",
        "    'description': ['', '', '', '', ''],\n",
        "    'frequency': ['monthly', 'monthly', 'annual', 'quarterly', 'monthly'],\n",
        "    'start': ['2001-01', '2001-01', '2001-01', '2001-Q1', '2001-01'],\n",
        "    'end': ['2024-12', '2024-12', '2024-12', '2024-Q4', '2024-12'],\n",
        "    'url': ['', '', '', '', '']\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "table_df = pd.DataFrame(tables)"
      ],
      "metadata": {
        "id": "CZWpGZEIM-1x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and run the ETL pipeline\n",
        "pipeline = StatCanETL(table_df)\n",
        "pipeline.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjqkOZPWQJrA",
        "outputId": "1cb0c4bf-b8b2-42d7-80c0-9dccb45bc353"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing wage...\n",
            "Data for wage saved to object wage_df\n",
            "Processing raw...\n",
            "Data for raw saved to object raw_df\n",
            "Processing tax...\n",
            "Data for tax saved to object tax_df\n",
            "Processing productivity...\n",
            "Data for productivity saved to object productivity_df\n",
            "Processing cpi...\n",
            "Data for cpi saved to object cpi_df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA\n",
        "\n",
        "**check to see if the data loaded correctly:**"
      ],
      "metadata": {
        "id": "LzTno-zYw-zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in pipeline.data_objects.items():\n",
        "    print(key)\n",
        "    print(value.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlIzORgxQIJW",
        "outputId": "f096c62b-96a6-4bb4-e80b-129f155b7fe9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wage_df\n",
            "      Date   Value\n",
            "0  2001-01  657.14\n",
            "1  2001-02  653.59\n",
            "2  2001-03  655.14\n",
            "raw_df\n",
            "      Date  Value\n",
            "0  2001-01   65.2\n",
            "1  2001-02   65.8\n",
            "2  2001-03   65.5\n",
            "tax_df\n",
            "   Date  Value\n",
            "0  2001   14.0\n",
            "1  2002   13.8\n",
            "2  2003   13.8\n",
            "productivity_df\n",
            "      Date   Value\n",
            "0  2001-01  85.173\n",
            "1  2001-04  86.150\n",
            "2  2001-07  86.665\n",
            "cpi_df\n",
            "      Date  Value\n",
            "0  2001-01   96.3\n",
            "1  2001-02   96.8\n",
            "2  2001-03   97.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge tables"
      ],
      "metadata": {
        "id": "8vZv0MPV1A3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.merge(wage_df, raw_df,\n",
        "                on = 'Date',\n",
        "                how = 'outer')\n",
        "\n",
        "data = pd.merge(data, tax_df,\n",
        "                on = 'Date',\n",
        "                how = 'outer')\n",
        "\n",
        "data = pd.merge(data, productivity_df,\n",
        "                on = 'Date',\n",
        "                how = 'outer')\n",
        "\n",
        "data = pd.merge(data, cpi_df,\n",
        "                on = 'Date',\n",
        "                how = 'outer')"
      ],
      "metadata": {
        "id": "FHNrv2-qM_GO",
        "outputId": "e5744f72-0c79-4eac-99be-1d0f147133b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MergeError",
          "evalue": "Passing 'suffixes' which cause duplicate columns {'Value_x'} is not allowed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-76957d8da2ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                 how = 'outer')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m data = pd.merge(data, productivity_df,\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 how = 'outer')\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         result = self._reindex_and_concat(\n\u001b[0m\u001b[1;32m    889\u001b[0m             \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[0m\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[0;34m(left, right, suffixes)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mdups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m         raise MergeError(\n\u001b[0m\u001b[1;32m   2758\u001b[0m             \u001b[0;34mf\"Passing 'suffixes' which cause duplicate columns {set(dups)} is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m             \u001b[0;34mf\"not allowed.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'Value_x'} is not allowed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6m0xBoJyM_Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oy_B9q9AM_Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEWPpkkBM_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dmdMt29M_Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFi-hkmCM_V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHxH-BsxyMSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixEQ08jMM_ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "JrhHrKWvyNsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Statistics Canada SDMX userguide](https://www.statcan.gc.ca/en/developers/sdmx/user-guide)\n",
        "2. [OpenAI](https://chatgpt.com/)"
      ],
      "metadata": {
        "id": "kqH1IVYPySew"
      }
    }
  ]
}